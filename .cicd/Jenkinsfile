pipeline {
    agent none

    options {
        disableConcurrentBuilds()
        overrideIndexTriggers(false)
        skipDefaultCheckout(true)
        timestamps()
        timeout(time: 12, unit: 'HOURS')
    }

    parameters {
        // Allow job runner to filter based on platform
        // Use the line below to enable all PW clusters
        // choice(name: 'SRW_PLATFORM_FILTER', choices: ['all', 'derecho', 'gaea', 'hera', 'jet', 'orion', 'hercules', 'pclusternoaav2use1', 'azclusternoaav2eus1', 'gclusternoaav2usc1'], description: 'Specify the platform(s) to use')
        // Use the line below to enable the PW AWS cluster
        // choice(name: 'SRW_PLATFORM_FILTER', choices: ['all', 'derecho', 'gaea', 'hera', 'jet', 'orion', 'hercules', 'pclusternoaav2use1'], description: 'Specify the platform(s) to use')
        // Use the line below to enable Derecho once the project allocation is available
        // choice(name: 'SRW_PLATFORM_FILTER', choices: ['all', 'derecho', 'gaea', 'hera', 'jet', 'orion', 'hercules'], description: 'Specify the platform(s) to use')
        choice(name: 'SRW_PLATFORM_FILTER', choices: ['all', 'gaea', 'hera', 'jet', 'orion', 'hercules'], description: 'Specify the platform(s) to use')
        // Allow job runner to filter based on compiler
        choice(name: 'SRW_COMPILER_FILTER', choices: ['all', 'gnu', 'intel'], description: 'Specify the compiler(s) to use to build')
        // Workflow Wrapper test depth {0..9}, 0=none, 1=simple, 9=all [default]
        choice(name: 'SRW_WRAPPER_TASK_DEPTH', choices: ['9', '1', '0'], description: '0=none, 1=simple, 9=all [default]')
        // WE2E Tests ?
        choice(name: 'SRW_WE2E_SINGLE_TEST', choices: ['coverage', 'none', 'skill-score', 'grid_SUBCONUS_Ind_3km_ics_FV3GFS_lbcs_FV3GFS_suite_WoFS_v0'], description: 'Specify the WE2E test to use')
        booleanParam name: 'SRW_WE2E_COMPREHENSIVE_TESTS', defaultValue: false, description: 'Whether to execute the comprehensive end-to-end tests'
    }

    stages {
        stage('Launch SonarQube') {
            steps {
                script {
                    echo "BRANCH_NAME=${env.CHANGE_BRANCH}"
                    echo "FORK_NAME=${env.CHANGE_FORK}"
                    echo "CHANGE_URL=${env.CHANGE_URL}"
                    echo "CHANGE_ID=${env.CHANGE_ID}"
                    build job: '/ufs-srweather-app/ufs-srw-sonarqube', parameters: [
                        string(name: 'BRANCH_NAME', value: env.CHANGE_BRANCH ?: 'develop'),
                        string(name: 'FORK_NAME', value: env.CHANGE_FORK ?: ''),
                        string(name: 'CHANGE_URL', value: env.CHANGE_URL ?: ''),
                        string(name: 'CHANGE_ID', value: env.CHANGE_ID ?: '')
                    ], wait: false
                }
            }
        }

        // Uncomment the following block to re-enable PW clusters
        /*
        // Start the NOAA Parallel Works clusters, if necessary
        stage('Start Parallel Works Clusters') {
            matrix {
                // Start all clusters by default or only the specified cluster given by SRW_PLATFORM_FILTER
                when {
                    anyOf {
                        expression { params.SRW_PLATFORM_FILTER == 'all' }
                        expression { params.SRW_PLATFORM_FILTER == env.SRW_PLATFORM }
                    }
                }

                axes {
                    axis {
                        name 'SRW_PLATFORM'
                        values 'pclusternoaav2use1' //, 'azclusternoaav2eus1', 'gclusternoaav2usc1'
                    }
                }

                stages {
                    // Call the parallel-works-jenkins-client/start-cluster job using SRW_PLATFORM for the
                    // PW_CLUSTER_NAME parameter
                    stage('Start Cluster') {
                        steps {
                            build job: 'parallel-works-jenkins-client/start-cluster', parameters: [string(name: 'PW_CLUSTER_NAME', value: env.SRW_PLATFORM), string(name: 'PW_CLUSTER_SSH_KEY', value: '~/.ssh/id_rsa'), string(name: 'JAVA_VERSION', value: '11')]
                        }
                    }
                }
            }
        }
        */

        // Build and test the SRW application on all supported platforms using the supported compilers for each platform
        stage('Build and Test') {
            matrix {
                // Run on all platform/compiler combinations by default or build and test only on the platform(s) and
                // compiler(s) specified by SRW_PLATFORM_FILTER and SRW_COMPILER_FILTER
                when {
                    beforeAgent true
                    expression {
                        return nodesByLabel(env.SRW_PLATFORM).size() > 0
                    }

                    allOf {
                        anyOf {
                            expression { params.SRW_PLATFORM_FILTER == 'all' }
                            expression { params.SRW_PLATFORM_FILTER == env.SRW_PLATFORM }
                        }

                        anyOf {
                            expression { params.SRW_COMPILER_FILTER == 'all' }
                            expression { params.SRW_COMPILER_FILTER == env.SRW_COMPILER }
                        }
                    }
                }

                axes {
                    axis {
                        name 'SRW_PLATFORM'
                        // values 'derecho', 'gaea', 'hera', 'jet', 'orion', 'hercules' //, 'pclusternoaav2use1', 'azclusternoaav2eus1', 'gclusternoaav2usc1'
                        values 'gaea', 'hera', 'jet', 'orion', 'hercules' //, 'pclusternoaav2use1', 'azclusternoaav2eus1', 'gclusternoaav2usc1'
                    }

                    axis {
                        name 'SRW_COMPILER'
                        values 'gnu', 'intel'
                    }
                }

                excludes {
                    // Exclude GNU from platforms that don't support it
                    exclude {
                            axis {
                            name 'SRW_PLATFORM'
                            // values 'derecho', 'gaea', 'jet', 'orion', 'hercules' //, 'pclusternoaav2use1' , 'azclusternoaav2eus1', 'gclusternoaav2usc1'
                            values 'gaea', 'jet', 'orion', 'hercules' //, 'pclusternoaav2use1' , 'azclusternoaav2eus1', 'gclusternoaav2usc1'
                        }

                        axis {
                            name 'SRW_COMPILER'
                            values 'gnu'
                        }
                    }
                }

                agent {
                    label env.SRW_PLATFORM
                }

                environment {
                    BRANCH_NAME_ESCAPED = env.BRANCH_NAME.replace('/', '_')
                    BUILD_VERSION = "${env.SRW_PLATFORM}-${env.SRW_COMPILER}-${env.BRANCH_NAME_ESCAPED}-${env.BUILD_NUMBER}"
                    BUILD_NAME = "ufs-srweather-app_${env.BUILD_VERSION}"
                    INSTALL_NAME = "install_${env.SRW_COMPILER}"
                }

                stages {
                    // Clean the workspace, checkout the repository, and run checkout_externals
                    stage('Initialize') {
                        steps {
                            dir ("${env.SRW_PLATFORM}") {
                            echo "${env.STAGE_NAME} SRW (${env.SRW_COMPILER}) build environment on ${env.SRW_PLATFORM} (using ${env.WORKSPACE}/${env.SRW_PLATFORM})"
                            cleanWs()
                            checkout scm
                            sh '"${WORKSPACE}/${SRW_PLATFORM}/.cicd/scripts/srw_init.sh"'
                            sh "STAGE_NAME=${env.STAGE_NAME} " + 'bash --login "${WORKSPACE}/${SRW_PLATFORM}/.cicd/scripts/disk_usage.sh"'
                            }
                        }
                        
                        post {
                            always {
                                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: false, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}-*-time-srw_init.json", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 'main', userMetadata: []
                                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: false, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}-*-disk-usage${env.STAGE_NAME}.csv", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 'main', userMetadata: []
                            }
                        }
                    }

                    // Run the unified build script; if successful create a tarball of the build and upload to S3
                    stage('Build') {
                        options {
                            timeout(time: 4, unit: 'HOURS')
                        }
                        
                        steps {
                            dir ("${env.SRW_PLATFORM}") {
                            echo "${env.STAGE_NAME} SRW (${env.SRW_COMPILER}) on ${env.SRW_PLATFORM} (using ${env.WORKSPACE}/${env.SRW_PLATFORM})"
                            sh 'bash --login "${WORKSPACE}/${SRW_PLATFORM}/.cicd/scripts/srw_build.sh"'
                            sh "STAGE_NAME=${env.STAGE_NAME} " + 'bash --login "${WORKSPACE}/${SRW_PLATFORM}/.cicd/scripts/disk_usage.sh"'
                            }
                        }

                        post {
                            success {
                                sh 'cd "${WORKSPACE}/${SRW_PLATFORM}/${INSTALL_NAME}" && tar --create --gzip --verbose --file "${WORKSPACE}/${SRW_PLATFORM}/${BUILD_NAME}.tgz" *'
                                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: true, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}/${env.BUILD_NAME}.tgz", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false], [bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: true, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}/build_${env.SRW_COMPILER}/srw_build-${env.SRW_PLATFORM}-${env.SRW_COMPILER}.txt", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 'main', userMetadata: []
                            }
                            always {
                                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: false, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}-*-env.txt", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 'main', userMetadata: []
                                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: false, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}-*-time-srw_build.json", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 'main', userMetadata: []
                                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: false, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}-*-disk-usage${env.STAGE_NAME}.csv", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 'main', userMetadata: []
                            }
                        }
                    }

                    // Run the unittest functional tests that require an HPC platform
                    stage('Functional UnitTests') {
                        steps {
                            dir ("${env.SRW_PLATFORM}") {
                            echo "Running unittest on retrieve_data.py"
                            sh 'bash --login "${WORKSPACE}/${SRW_PLATFORM}/.cicd/scripts/srw_unittest.sh"'
                            }
                        }
                    }
                    
                    // Try a few Workflow Task scripts to make sure E2E tests can be launched in a follow-on 'Test' stage
                    stage('Functional WorkflowTaskTests') {
                        environment {
                            TASK_DEPTH = "${params.SRW_WRAPPER_TASK_DEPTH}"
                        }

                        steps {
                            dir ("${env.SRW_PLATFORM}") {
                            echo "Running ${TASK_DEPTH} simple workflow script task tests on ${env.SRW_PLATFORM} (using ${env.WORKSPACE}/${env.SRW_PLATFORM})"
                            sh 'bash --login "${WORKSPACE}/${SRW_PLATFORM}/.cicd/scripts/wrapper_srw_ftest.sh"'
                            }
                        }
                    }

                    // Run the unified test script
                    stage('Test') {
                        options {
                            timeout(time: 8, unit: 'HOURS')
                        }
                        
                        environment {
                            SRW_WE2E_EXPERIMENT_BASE_DIR = "${env.WORKSPACE}/${env.SRW_PLATFORM}/expt_dirs"
                        }

                        steps {
                            dir ("${env.SRW_PLATFORM}") {
                            echo "${env.STAGE_NAME} SRW (${env.SRW_COMPILER}) on ${env.SRW_PLATFORM} (using ${env.WORKSPACE}/${env.SRW_PLATFORM})"

                            // If executing for a Pull Request, check for the run_we2e_comprehensive_tests. If set,
                            // override the value of the SRW_WE2E_COMPREHENSIVE_TESTS parameter
                            script {
                                def single_test = params.SRW_WE2E_SINGLE_TEST
                                def run_we2e_comprehensive_tests = params.SRW_WE2E_COMPREHENSIVE_TESTS
                                def run_we2e_comprehensive_tests_label = 'run_we2e_comprehensive_tests'

                                if (env.CHANGE_ID) {
                                    pullRequest.labels.each {
                                        if (it == run_we2e_comprehensive_tests_label) {
                                            run_we2e_comprehensive_tests = true
                                        }
                                    }
                                }

                                sh "SRW_WE2E_COMPREHENSIVE_TESTS=${run_we2e_comprehensive_tests} SRW_WE2E_SINGLE_TEST=${single_test}" + ' bash --login "${WORKSPACE}/${SRW_PLATFORM}/.cicd/scripts/srw_test.sh"'

                                }
                                sh "STAGE_NAME=${env.STAGE_NAME} " + 'bash --login "${WORKSPACE}/${SRW_PLATFORM}/.cicd/scripts/disk_usage.sh"'
                            }
                        }

                        post {
                            success {
                                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: false, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}/*-skill-score.txt", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 'main', userMetadata: []
                            }
                            always {
                                // Archive the test log files
                                sh "[[ -d ${SRW_WE2E_EXPERIMENT_BASE_DIR} ]] && cd ${SRW_WE2E_EXPERIMENT_BASE_DIR} && tar --create --gzip --verbose --dereference --file ${env.WORKSPACE}/${env.SRW_PLATFORM}/we2e_test_logs-${env.SRW_PLATFORM}-${env.SRW_COMPILER}.tgz */log.generate_FV3LAM_wflow */log/* ${env.WORKSPACE}/${env.SRW_PLATFORM}/tests/WE2E/WE2E_tests_*yaml WE2E_summary*txt ${env.WORKSPACE}/${env.SRW_PLATFORM}/tests/WE2E/log.* || cat /dev/null > ${env.WORKSPACE}/${env.SRW_PLATFORM}/we2e_test_logs-${env.SRW_PLATFORM}-${env.SRW_COMPILER}.tgz"
                                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: false, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}/*_test_results-*-*.txt", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false], [bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: false, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}/we2e_test_logs-${env.SRW_PLATFORM}-${env.SRW_COMPILER}.tgz", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 'main', userMetadata: []
                                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: false, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}-*-time-srw_test.json", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 'main', userMetadata: []
                                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'noaa-epic-prod-jenkins-artifacts', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: true, noUploadOnFailure: false, selectedRegion: 'us-east-1', showDirectlyInBrowser: false, sourceFile: "${env.SRW_PLATFORM}-*-disk-usage${env.STAGE_NAME}.csv", storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 'main', userMetadata: []
                                // Remove the data sets from the experiments directory to conserve disk space
                                sh 'find "${SRW_WE2E_EXPERIMENT_BASE_DIR}" -regextype posix-extended -regex "^.*(orog|[0-9]{10})$" -type d | xargs rm -rf'
                            }
                        }
                    }
                }
            }
        }
    }
    // end of stages{}

    post {
        always {
            script {
                // Trigger another job to collect all build statistics
                CI_JOB_NAME=env.JOB_NAME.replace("/${env.JOB_BASE_NAME}","")
                CI_BRANCH_NAME=env.JOB_BASE_NAME.replace("%2F","%252F")
                echo "post: Triggering ufs-srweather-app/ufs-srw-metrics job for ${CI_JOB_NAME} on branch build ${CI_BRANCH_NAME}/${env.BUILD_NUMBER} ..."
                build job: '/ufs-srweather-app/ufs-srw-metrics', parameters: [
                        string(name: 'CI_JOB_NAME', value: "${CI_JOB_NAME}"),
                        string(name: 'CI_BUILD_NUMBER', value: "${CI_BRANCH_NAME}/${env.BUILD_NUMBER}")
                ], wait: false

    // Uncomment the following block to re-enable PW clusters
    /*
            // Stop any Parallel Works clusters that were started during the pipeline execution
                // def pw_clusters = ['pclusternoaav2use1', 'azclusternoaav2eus1', 'gclusternoaav2usc1']
                def pw_clusters = ['pclusternoaav2use1']
                def clusters = []

                // Determine which clusters need to be stopped, if any
                if (params.SRW_PLATFORM_FILTER == 'all') {
                    clusters = pw_clusters
                } else if (params.SRW_PLATFORM_FILTER in pw_clusters) {
                    clusters = [params.SRW_PLATFORM_FILTER]
                } else {
                    echo 'No Parallel Works clusters were used in build'
                }

                for (int i = 0; i < clusters.size(); ++i) {
                    // Call the parallel-works-jenkins-client/stop-cluster job using clusters[i] for the
                    // PW_CLUSTER_NAME parameter
                    build job: 'parallel-works-jenkins-client/stop-cluster', parameters: [string(name: 'PW_CLUSTER_NAME', value: clusters[i])]
                }
    */          
            }
        }
    }
}
